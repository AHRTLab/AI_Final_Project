{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LN0nZwyMGadB"
   },
   "source": [
    "# Sonic The Hedgehog 1\n",
    "\n",
    "## Step 1: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import retro\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro.__file__\n",
    "\"\"\"Cd to the directory and copy the rom.md file into the sonic_genesis directory\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from algos.agents.dqn_agent import DQNAgent\n",
    "from algos.preprocessing.stack_frame import preprocess_frame, stack_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Step: Play Sonic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Interact with Gym environments using the keyboard\n",
    "\n",
    "An adapter object is defined for each environment to map keyboard commands to actions and extract observations as pixels.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import ctypes\n",
    "import argparse\n",
    "import abc\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import retro\n",
    "import pyglet\n",
    "from pyglet import gl\n",
    "from pyglet.window import key as keycodes\n",
    "\n",
    "\n",
    "class Interactive(abc.ABC):\n",
    "    \"\"\"\n",
    "    Base class for making gym environments interactive for human use\n",
    "    \"\"\"\n",
    "    def __init__(self, env, sync=True, tps=60, aspect_ratio=None):\n",
    "        obs = env.reset()\n",
    "        self._image = self.get_image(obs, env)\n",
    "        assert len(self._image.shape) == 3 and self._image.shape[2] == 3, 'must be an RGB image'\n",
    "        image_height, image_width = self._image.shape[:2]\n",
    "\n",
    "        if aspect_ratio is None:\n",
    "            aspect_ratio = image_width / image_height\n",
    "\n",
    "        # guess a screen size that doesn't distort the image too much but also is not tiny or huge\n",
    "        display = pyglet.canvas.get_display()\n",
    "        screen = display.get_default_screen()\n",
    "        max_win_width = screen.width * 0.9\n",
    "        max_win_height = screen.height * 0.9\n",
    "        win_width = image_width\n",
    "        win_height = int(win_width / aspect_ratio)\n",
    "\n",
    "        while win_width > max_win_width or win_height > max_win_height:\n",
    "            win_width //= 2\n",
    "            win_height //= 2\n",
    "        while win_width < max_win_width / 2 and win_height < max_win_height / 2:\n",
    "            win_width *= 2\n",
    "            win_height *= 2\n",
    "\n",
    "        win = pyglet.window.Window(width=win_width, height=win_height)\n",
    "\n",
    "        self._key_handler = pyglet.window.key.KeyStateHandler()\n",
    "        win.push_handlers(self._key_handler)\n",
    "        win.on_close = self._on_close\n",
    "\n",
    "        gl.glEnable(gl.GL_TEXTURE_2D)\n",
    "        self._texture_id = gl.GLuint(0)\n",
    "        gl.glGenTextures(1, ctypes.byref(self._texture_id))\n",
    "        gl.glBindTexture(gl.GL_TEXTURE_2D, self._texture_id)\n",
    "        gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_WRAP_S, gl.GL_CLAMP)\n",
    "        gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_WRAP_T, gl.GL_CLAMP)\n",
    "        gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n",
    "        gl.glTexParameteri(gl.GL_TEXTURE_2D, gl.GL_TEXTURE_MIN_FILTER, gl.GL_NEAREST)\n",
    "        gl.glTexImage2D(gl.GL_TEXTURE_2D, 0, gl.GL_RGBA8, image_width, image_height, 0, gl.GL_RGB, gl.GL_UNSIGNED_BYTE, None)\n",
    "\n",
    "        self._env = env\n",
    "        self._win = win\n",
    "\n",
    "        # self._render_human = render_human\n",
    "        self._key_previous_states = {}\n",
    "\n",
    "        self._steps = 0\n",
    "        self._episode_steps = 0\n",
    "        self._episode_returns = 0\n",
    "        self._prev_episode_returns = 0\n",
    "\n",
    "        self._tps = tps\n",
    "        self._sync = sync\n",
    "        self._current_time = 0\n",
    "        self._sim_time = 0\n",
    "        self._max_sim_frames_per_update = 4\n",
    "\n",
    "    def _update(self, dt):\n",
    "        # cap the number of frames rendered so we don't just spend forever trying to catch up on frames\n",
    "        # if rendering is slow\n",
    "        max_dt = self._max_sim_frames_per_update / self._tps\n",
    "        if dt > max_dt:\n",
    "            dt = max_dt\n",
    "\n",
    "        # catch up the simulation to the current time\n",
    "        self._current_time += dt\n",
    "        while self._sim_time < self._current_time:\n",
    "            self._sim_time += 1 / self._tps\n",
    "\n",
    "            keys_clicked = set()\n",
    "            keys_pressed = set()\n",
    "            for key_code, pressed in self._key_handler.items():\n",
    "                if pressed:\n",
    "                    keys_pressed.add(key_code)\n",
    "\n",
    "                if not self._key_previous_states.get(key_code, False) and pressed:\n",
    "                    keys_clicked.add(key_code)\n",
    "                self._key_previous_states[key_code] = pressed\n",
    "\n",
    "            if keycodes.ESCAPE in keys_pressed:\n",
    "                self._on_close()\n",
    "\n",
    "            # assume that for async environments, we just want to repeat keys for as long as they are held\n",
    "            inputs = keys_pressed\n",
    "            if self._sync:\n",
    "                inputs = keys_clicked\n",
    "\n",
    "            keys = []\n",
    "            for keycode in inputs:\n",
    "                for name in dir(keycodes):\n",
    "                    if getattr(keycodes, name) == keycode:\n",
    "                        keys.append(name)\n",
    "\n",
    "            act = self.keys_to_act(keys)\n",
    "\n",
    "            if not self._sync or act is not None:\n",
    "                obs, rew, done, _info = self._env.step(act)\n",
    "                self._image = self.get_image(obs, self._env)\n",
    "                self._episode_returns += rew\n",
    "                self._steps += 1\n",
    "                self._episode_steps += 1\n",
    "                np.set_printoptions(precision=2)\n",
    "                if self._sync:\n",
    "                    done_int = int(done)  # shorter than printing True/False\n",
    "                    mess = 'steps={self._steps} episode_steps={self._episode_steps} rew={rew} episode_returns={self._episode_returns} done={done_int}'.format(\n",
    "                        **locals()\n",
    "                    )\n",
    "                    print(mess)\n",
    "                elif self._steps % self._tps == 0 or done:\n",
    "                    episode_returns_delta = self._episode_returns - self._prev_episode_returns\n",
    "                    self._prev_episode_returns = self._episode_returns\n",
    "                    mess = 'steps={self._steps} episode_steps={self._episode_steps} episode_returns_delta={episode_returns_delta} episode_returns={self._episode_returns}'.format(\n",
    "                        **locals()\n",
    "                    )\n",
    "                    print(mess)\n",
    "\n",
    "                if done:\n",
    "                    self._env.reset()\n",
    "                    self._episode_steps = 0\n",
    "                    self._episode_returns = 0\n",
    "                    self._prev_episode_returns = 0\n",
    "\n",
    "    def _draw(self):\n",
    "        gl.glBindTexture(gl.GL_TEXTURE_2D, self._texture_id)\n",
    "        video_buffer = ctypes.cast(self._image.tobytes(), ctypes.POINTER(ctypes.c_short))\n",
    "        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, 0, 0, self._image.shape[1], self._image.shape[0], gl.GL_RGB, gl.GL_UNSIGNED_BYTE, video_buffer)\n",
    "\n",
    "        x = 0\n",
    "        y = 0\n",
    "        w = self._win.width\n",
    "        h = self._win.height\n",
    "\n",
    "        pyglet.graphics.draw(\n",
    "            4,\n",
    "            pyglet.gl.GL_QUADS,\n",
    "            ('v2f', [x, y, x + w, y, x + w, y + h, x, y + h]),\n",
    "            ('t2f', [0, 1, 1, 1, 1, 0, 0, 0]),\n",
    "        )\n",
    "\n",
    "    def _on_close(self):\n",
    "        self._env.close()\n",
    "        sys.exit(0)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_image(self, obs, venv):\n",
    "        \"\"\"\n",
    "        Given an observation and the Env object, return an rgb array to display to the user\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def keys_to_act(self, keys):\n",
    "        \"\"\"\n",
    "        Given a list of keys that the user has input, produce a gym action to pass to the environment\n",
    "\n",
    "        For sync environments, keys is a list of keys that have been pressed since the last step\n",
    "        For async environments, keys is a list of keys currently held down\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the interactive window until the user quits\n",
    "        \"\"\"\n",
    "        # pyglet.app.run() has issues like https://bitbucket.org/pyglet/pyglet/issues/199/attempting-to-resize-or-close-pyglet\n",
    "        # and also involves inverting your code to run inside the pyglet framework\n",
    "        # avoid both by using a while loop\n",
    "        prev_frame_time = time.time()\n",
    "        while True:\n",
    "            self._win.switch_to()\n",
    "            self._win.dispatch_events()\n",
    "            now = time.time()\n",
    "            self._update(now - prev_frame_time)\n",
    "            prev_frame_time = now\n",
    "            self._draw()\n",
    "            self._win.flip()\n",
    "\n",
    "\n",
    "class RetroInteractive(Interactive):\n",
    "    \"\"\"\n",
    "    Interactive setup for retro games\n",
    "    \"\"\"\n",
    "    def __init__(self, game, state, scenario):\n",
    "        env = retro.make(game=game, state=state, scenario=scenario)\n",
    "        self._buttons = env.buttons\n",
    "        super().__init__(env=env, sync=False, tps=60, aspect_ratio=4/3)\n",
    "\n",
    "    def get_image(self, _obs, env):\n",
    "        return env.render(mode='rgb_array')\n",
    "\n",
    "    def keys_to_act(self, keys):\n",
    "        inputs = {\n",
    "            None: False,\n",
    "\n",
    "            'BUTTON': 'Z' in keys,\n",
    "            'A': 'Z' in keys,\n",
    "            'B': 'X' in keys,\n",
    "\n",
    "            'C': 'C' in keys,\n",
    "            'X': 'A' in keys,\n",
    "            'Y': 'S' in keys,\n",
    "            'Z': 'D' in keys,\n",
    "\n",
    "            'L': 'Q' in keys,\n",
    "            'R': 'W' in keys,\n",
    "\n",
    "            'UP': 'UP' in keys,\n",
    "            'DOWN': 'DOWN' in keys,\n",
    "            'LEFT': 'LEFT' in keys,\n",
    "            'RIGHT': 'RIGHT' in keys,\n",
    "\n",
    "            'MODE': 'TAB' in keys,\n",
    "            'SELECT': 'TAB' in keys,\n",
    "            'RESET': 'ENTER' in keys,\n",
    "            'START': 'ENTER' in keys,\n",
    "        }\n",
    "        return [inputs[b] for b in self._buttons]\n",
    "\n",
    "ia = RetroInteractive(game='SonicTheHedgehog-Genesis', state=retro.State.DEFAULT, scenario=None)\n",
    "ia.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tfo8jleHGadK"
   },
   "source": [
    "## Step 2: Create our environment\n",
    "\n",
    "Initialize the environment in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game='SonicTheHedgehog-Genesis', state='GreenHillZone.Act1', scenario='contest')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nS221MgXGadP"
   },
   "source": [
    "## Step 3: Viewing our Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of frame is: \", env.observation_space.shape)\n",
    "print(\"No. of Actions: \", env.action_space.n)\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(env.reset())\n",
    "plt.title('Original Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = {\n",
    "            # No Operation\n",
    "            0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            # Left\n",
    "            1: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            # Right\n",
    "            2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            # Left, Down\n",
    "            3: [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "            # Right, Down\n",
    "            4: [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "            # Down\n",
    "            5: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            # Down, B\n",
    "            6: [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            # B\n",
    "            7: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the code cell below to play Sonicwith a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play():\n",
    "    score = 0\n",
    "    env.viewer = None\n",
    "    env.reset()\n",
    "    for i in range(200):\n",
    "        env.render()\n",
    "        action = possible_actions[np.random.randint(len(possible_actions))]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        if done:\n",
    "            print(\"Your Score at end of game is: \", score)\n",
    "            break\n",
    "    env.reset()\n",
    "    env.render(close=True)\n",
    "random_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sr52nmcpGada"
   },
   "source": [
    "## Step 4:Preprocessing Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(preprocess_frame(env.reset(), (1, -1, -1, 1), 84), cmap=\"gray\")\n",
    "plt.title('Pre Processed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJMc3HA8Gade"
   },
   "source": [
    "## Step 5: Stacking Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(frames, state, is_new=False):\n",
    "    frame = preprocess_frame(state, (1, -1, -1, 1), 84)\n",
    "    frames = stack_frame(frames, frame, is_new)\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''Create your model Here'''\n",
    "class DQNCnn(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQNCnn, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            '''your code here'''\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = len(possible_actions)\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "BUFFER_SIZE = 100000   # replay buffer size\n",
    "BATCH_SIZE = 32        # Update batch size\n",
    "LR = 0.0001            # learning rate \n",
    "TAU = 1e-3             # for soft update of target parameters\n",
    "UPDATE_EVERY = 100     # how often to update the network\n",
    "UPDATE_TARGET = 10000  # After which thershold replay to be started \n",
    "EPS_START = 0.99       # starting value of epsilon\n",
    "EPS_END = 0.01         # Ending value of epsilon\n",
    "EPS_DECAY = 100         # Rate by which epsilon to be decayed\n",
    "agent = DQNAgent(INPUT_SHAPE, \n",
    "                 ACTION_SIZE, \n",
    "                 SEED, \n",
    "                 device, \n",
    "                 BUFFER_SIZE, \n",
    "                 BATCH_SIZE, \n",
    "                 GAMMA, \n",
    "                 LR, \n",
    "                 TAU, \n",
    "                 UPDATE_EVERY, \n",
    "                 UPDATE_TARGET, \n",
    "                 DQNCnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Watching untrained agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.viewer = None\n",
    "# watch an untrained agent\n",
    "state = stack_frames(None, env.reset(), True) \n",
    "for j in range(1000):\n",
    "    env.render(close=False)\n",
    "    action = agent.act(state, eps=0.01)\n",
    "    next_state, reward, done, _ = env.step(possible_actions[action])\n",
    "    state = stack_frames(state, next_state, False)\n",
    "    if done:\n",
    "        env.reset()\n",
    "        break \n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train the Agent with DQN\n",
    "Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "scores = []\n",
    "scores_window = deque(maxlen=20)\n",
    "epsilon_by_epsiode = lambda frame_idx: EPS_END + (EPS_START - EPS_END) * math.exp(-1. * frame_idx /EPS_DECAY)\n",
    "plt.plot([epsilon_by_epsiode(i) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=1):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    save_reward = 2000\n",
    "    for i_episode in range(start_epoch + 1, n_episodes+1):\n",
    "        state = stack_frames(None, env.reset(), True)\n",
    "        score = 0\n",
    "        eps = epsilon_by_epsiode(i_episode)\n",
    "\n",
    "        # Punish the agent for not moving forward\n",
    "        prev_state = {}\n",
    "        steps_stuck = 0\n",
    "        timestamp = 0\n",
    "\n",
    "        while timestamp < 10000:\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, info = env.step(possible_actions[action])\n",
    "            score += reward\n",
    "\n",
    "            timestamp += 1\n",
    "\n",
    "            # Punish the agent for standing still for too long.\n",
    "            if (prev_state == info):\n",
    "                steps_stuck += 1\n",
    "            else:\n",
    "                steps_stuck = 0\n",
    "            prev_state = info\n",
    "    \n",
    "            if (steps_stuck > 20):\n",
    "                reward -= 1\n",
    "            \n",
    "            next_state = stack_frames(state, next_state, False)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        \n",
    "        if score > save_reward:\n",
    "            torch.save(agent.policy_net.state_dict(), 'Saved_Models/policy_net_' + str(score))\n",
    "            torch.save(agent.target_net.state_dict(), 'Saved_Models/target_net_' + str(score))\n",
    "            save_reward = score\n",
    "        clear_output(True)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(scores_window), eps), end=\"\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch your \"Smart\" Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# watch a trained agent\n",
    "env.viewer = None\n",
    "state = stack_frames(None, env.reset(), True) \n",
    "for j in range(10000):\n",
    "    env.render(close=False)\n",
    "    action = agent.act(state, eps=0)\n",
    "    next_state, reward, done, _ = env.step(possible_actions[action])\n",
    "    state = stack_frames(state, next_state, False)\n",
    "    time.sleep(0.01)\n",
    "    if done:\n",
    "        env.reset()\n",
    "        break \n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net.state_dict(), 'Saved_Models/rainbow')\n",
    "agent.policy_net.load_state_dict(torch.load('Saved_Models/rainbow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "space_invader.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
